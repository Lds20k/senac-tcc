\subsubsection*{Função de Ativação}

A função de ativação retorna a saída de um neurônio \cite{haykin1999neural}. Aqui, podem-se ver quatro tipos de funções de ativação:

\begin{enumerate}
	\item Função \textit{Sigmoid}, uma função não linear que produz uma curva com a forma de "S". É usada para mapear valores previstos em probabilidades e tem o valor de saída entre 0 e 1 \cite{gharat2019what}.
	\begin{figure}[ht]
	\caption{Gráfico da função \textit{Sigmoid}.}
	\begin{center}
		% Conteúdo existente
	\end{center}
	\legend{Fonte: Criação própria}
	\label{fig:grafico_sigmoid}
	\end{figure}
	Segundo \citeonline{gharat2019what}, a função \textit{Sigmoid} tem uma convergência lenta, é computacionalmente cara e, para valores muito extremos, causa problemas na previsão.

	\item Função \textit{ReLu} (Unidade Linear Retificada), uma função não linear inspirada nos neurônios do cérebro que retorna um valor positivo ou 0 \cite{rizzo2020inteligencia}.
	\begin{figure}[ht]
	\caption{Gráfico da função \textit{ReLu}.}
	\begin{center}
		% Conteúdo existente
	\end{center}
	\legend{Fonte: Criação própria}
	\label{fig:grafico_relu}
	\end{figure}
	A função \textit{ReLu} é computacionalmente eficiente e converge rapidamente; porém, quando a entrada da função se aproxima de zero, a rede neural não consegue executar a retropropagação, o que impede o aprendizado \cite{gharat2019what}.

	\item Função \textit{Leaky ReLU} (Unidade Linear Retificada com Vazamento), uma função não linear variante da \textit{ReLU}, que retorna um valor positivo ou $\upsilon/a_i$, sendo $a_i$ um valor na faixa $(1,\infty)$ \cite{xu2015empirical}.
	\begin{figure}[ht]
	\caption{Gráfico da função \textit{Leaky ReLU}.}
	\begin{center}
		% Conteúdo existente
	\end{center}
	\legend{Fonte: Criação própria}
	\label{fig:grafico_relu_leaky}
	\end{figure}
	Possui as mesmas características da função \textit{ReLU}, mas sem o problema da retropropagação \cite{gharat2019what}.

	\item Função \textit{Softmax}, que calcula a distribuição de probabilidades de um evento em "n" eventos e fornece a probabilidade do valor de entrada pertencer a uma classe específica. Geralmente é usada na camada de saída \cite{gharat2019what}.
	\begin{figure}[ht]
	\caption{Gráfico da função \textit{Softmax}.}
	\begin{center}
		% Conteúdo existente
	\end{center}
	\legend{Fonte: Criação própria}
	\label{fig:grafico_softmax}
	\end{figure}
\end{enumerate}

Com a função \textit{Softmax}, é possível normalizar a saída para valores entre 0 e 1 e calcular a probabilidade da entrada. Por causa dessas características, é utilizada na camada de saída da rede neural \cite{gharat2019what}.
