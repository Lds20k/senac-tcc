\subsubsection*{Função de perda}

A função de perda é calculada na camada de saída e serve para mensurar o sucesso obtido comparando com fórmulas o resultado da arquitetura com o resultado real do conjunto de dados. O resultado dessa função irá ajudar na retropropagação, \emph{i.e.}, servirá para ajustar os pesos e vieses da conexão entre os neurônios para minimizar o erro. A seguir algumas funções de perda, pontuando que todo esse subtópico é baseado em 
\citeonline{Alzubaidi2021}.

\subsubsection*{Softmax ou entropia cruzada ou logarítmica}

Muito utilizada para medir a performance de uma rede neural convolucional principalmente quando o resultado final têm várias classes. Antes dessa função de perda é necessário usar a função de ativação softmax descrita na \cref{fig:grafico_softmax} pois precisa de uma saída dentro de uma distribuição de probabilidade. Sendo N o número de classes ou o número de neurônios na camada de saída.

$$H(p,y) = -\sum_{i=1}^N y_i \log(p_i)$$

\subsubsection*{Euclidiana ou erro quadrático médio}

Muito utilizada para problemas de regressão.

$$H(p,y) = \frac{1}{2N} \sum_{i=1}^N (p_i - y_i)^2$$

\subsubsection*{Hinge}

Muito utilizado para classificação binária.

$$H(p,y) = \sum_{i=1}^N max(0, m - (2y_i - 1) p_i)$$
