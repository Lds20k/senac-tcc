\subsubsection*{Função de Perda}

A função de perda é calculada na camada de saída e serve para mensurar o sucesso obtido, comparando, com fórmulas, o resultado predito com o resultado real do conjunto de dados. O resultado dessa função irá ajudar na retropropagação, \emph{i.e.}, servirá para ajustar os pesos e vieses da conexão entre os neurônios, com o objetivo de minimizar o erro. A seguir, apresentam-se algumas funções de perda, pontuando que todo este subtópico é baseado em \citeonline{Alzubaidi2021}.

\subsubsubsection*{Softmax ou Entropia Cruzada ou Logarítmica}

Esta é muito utilizada para medir a performance de uma rede neural convolucional, principalmente quando o resultado envolve várias classes. Antes dessa função de perda, é necessário usar a função de ativação softmax, descrita na \cref{fig:grafico_softmax}, pois é preciso de uma saída dentro de uma distribuição de probabilidade. Sendo \( N \) o número de classes ou o número de neurônios na camada de saída.

\begin{equation} 
    H(p,y) = -\sum_{i=1}^{N} y_i \log(p_i)
\end{equation}


\subsubsubsection*{Euclidiana ou Erro Quadrático Médio}

Muito utilizada para problemas de regressão.

\begin{equation}
    H(p,y) = \frac{1}{2N} \sum_{i=1}^{N} (p_i - y_i)^2
\end{equation}

\subsubsubsection*{Hinge}

Utilizado frequentemente para classificação binária.

\begin{equation}
    H(p,y) = \sum_{i=1}^{N} \max(0, m - (2y_i - 1) p_i)
\end{equation}
