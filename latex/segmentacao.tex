\subsection{Segmentação}

O estudo de segmentação semântica dentro da área de redes neurais convolucionais têm três principais nichos, sendo eles: segmentação semântica que é a classifiicação por pixel, a segmentação de instância que atribui um id para cada objeto encontrado de uma classe, e a segmentação panóptica que junta as duas anteriores para criar uma imagem semelhante a saída de segmentação semântica porém separando objetos de mesma classe sendo essa a mais recente e completa, a diferença entre esses três tipos está ilustrado na \cref{fig:segentacoes} \cite{dp_semantic_segmantation, lapix}. 

\begin{figure}[H]
	\caption{Tipos de segmentação em redes neurais convolucionais}
	\centering % para centralizarmos a figura
	\includegraphics[width=10cm]{figures/segmantations.png} % leia abaixo
	\legend{Fonte: \citeonline{kirillov2019panoptic}}
	\label{fig:segentacoes}
\end{figure}

\subsubsection*{Segmentação semântica}

A segmentação semântica começou a ter resultados satisfatórios a partir de redes totalmente convolucionais, um modelo que descarta a camada totalmente conectada pois a saída deverá ser uma imagem e não uma classificação — isso a torna mais rápida para treinar do que as redes neurais convolucionais —, logo usa camadas deconvolucionais para transformar a matriz de características em uma imagem de qualquer dimensão na saída. A RTC criou a arquitetura chamada de salto (ou conexões) que serve para evitar perdas em camadas de agrupamento criando conexões entre camadas não consecutivas — geralmente entre camadas convolucionais e deconvolucionais — como apresentado na \cref{fig:rtc}, a arquitetura de salto evolui para arquitetura codificador-decodificador, o objetivo da RTC é segmentar imagens classificando pixels.
\begin{figure}[H]
	\caption{Exeplo de arquitetura de rede totalmente convolucional}
	\centering % para centralizarmos a figura
	\includegraphics[width=10cm]{figures/redes_totalmente_convolucionais.jpeg} % leia abaixo
	\legend{Fonte: \citeonline{dp_semantic_segmantation}}
	\label{fig:rtc}
\end{figure}

% A arquitetura codificador-decodificador — ou Encoder-Decoder — , que separa em dois passos: o primeiro para convergir no mapa de características — o modelo de redes totalmente convolucionais — e o segundo para reverter em uma imagem usando camadas deconvolucionais e de desagrupamento, como por exemplo a arquitetura UNet que foi a primeira a implementar o padrão Encoder-Decoder. Na \cref{} podemos observar que tem formato da letra U, sendo a descida a parte de Encoder e subida Decoder \cite{dp_semantic_segmantation, lapix}.

% \begin{figure}[H]
% 	\caption{Tipos de segmentação em redes neurais convolucionais}
% 	\centering % para centralizarmos a figura
% 	\includegraphics[width=10cm]{figures/segmantations.png} % leia abaixo
% 	\legend{Fonte: \citeonline{kirillov2019panoptic}}
% 	\label{fig:segentacoes}
% \end{figure}


\subsubsection*{Segmentação de instância}

% Mask R-CNN

\subsubsection*{Segmentação panóptica}

% https://www.v7labs.com/blog/panoptic-segmentation-guide#:~:text=In%20computer%20vision%2C%20the%20task%20of%20panoptic%20segmentation,a%20different%20color%E2%80%94%20labeling.%203%20Classifying%20the%20objects.

\subsubsection*{Modelo escolhido}

Com base no contexto acima percebemos que a segmentação panóptica é a mais completa e por efeito de estudos utilizaremos o mesmo para concluir o trabalho. Como nesse nicho existem várias alternativas abordaremos algumas métricas e resultados para selecionar o modelo.

\subsubsubsection*{União sobre intersecção}

A união sobre intersecção — Intersection over Union (IoU) — ou índice de Jaccard é uma métrica muito utlizada para calcular a eficiência de modelos de segmentação, ela se baseia em encontrar uma relação entre a área das classes da imagem de saída com as classes na imagem do conjunto de dados, segue a fôrmula \cite{dp_semantic_segmantation, lapix, kirillov2019panoptic}:

$$ 
IoU = \frac{\text{Área de intersecção}}{\text{Área de união}} \quad IoU(p_i, g) = \frac{p_i \cap g}{ p_i \cup g}
$$

% https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks
% https://lapix.ufsc.br/ensino/reconhecimento-de-padroes/avaliando-validando-e-testando-o-seu-modelo-metodologias-de-avaliacao-de-performance/

\subsubsubsection*{Qualidade panóptica}

A qualidade panóptica — ou Panoptic Quality (PQ) — foi definido pela primeira vez no artigo \citeonline{kirillov2019panoptic}, e se resume na formula:

\begin{equation}
\label{eq:pq_metric}
PQ = \frac{\sum_{(p,g)\in TP}IoU(p,g)}{ |TP| + \frac{1}{2}|FP| + \frac{1}{2}|FN|}
\end{equation}

Sendo TP True Positives ou Positivos Verdadeiros — quando existe intersecção significativa entre classes iguais, \emph{i.e.} IoU > 0.5 —,  FP é False Positives ou Falso Positivos — quando um objeto não é correspondido na imagem de predição — e FN é False Negatives ou Falso Negativos — quando um objeto não é correspondido na imagem do conjunto de dados — , podemos obsevar esses conceitos de conjuntos na \cref{fig:conjuntos}.
\begin{figure}[H]
	\caption{Exeplo da classificação dos conjuntos usados nas métricas de segmentação}
	\centering % para centralizarmos a figura
	\includegraphics[width=10cm]{figures/pan_metric.png} % leia abaixo
	\legend{Fonte: \citeonline{slidesKirillov}}
	\label{fig:conjuntos}
\end{figure}

Multiplicando a \cref{eq:pq_metric} por $\frac{|TP|}{|TP|}$ temos:

\begin{equation*}
	PQ = \underbrace{\frac{\sum_{(p,g)\in TP}IoU(p,g)}{|TP|}}_{\text{Segmentation Quality (SQ)}} 
	\times
	\underbrace{\frac{|TP|}{|TP| + \frac{1}{2}|FP| + \frac{1}{2}|FN|}}_{\text{Recognition Quality (RQ)}}
\end{equation*}

\subsubsubsection*{Resultados}

Os resultados são de uma competição em aberto criada pela Cytyscapes Dataset, essa competição tem várias modalidades e esses são referentes ao nicho de segmentação panóptica utilizando a métrica PG na classe de pessoas \cite{datasetResults}.
\begin{table}[H]
	\centering
	\caption{Top 15 modelos que melhor classificam pessoas}
	\label{tab:resultados-cityscapes}
	\begin{tabular}{|l|c|}
	  \hline
	  Nome do modelo & Precisão (\%) \\
	  \hline
	  EfficientPS [Mapillary Vistas] & 61,6 \\
	  EfficientPS [Cityscapes-fine] & 60,9 \\
	  Panoptic-DeepLab w/ SWideRNet [Mapillary Vistas + Pseudo-labels] & 60,6 \\
	  hri\_panoptic & 60,6 \\
	  Naive-Student (iterative semi-supervised learning with Panoptic-DeepLab) & 60,2 \\
	  Panoptic-DeepLab w/ SWideRNet [Mapillary Vistas] & 59,8 \\
	  iFLYTEK-CV & 59,2 \\
	  Panoptic-DeepLab [Mapillary Vistas] & 58,5 \\
	  Panoptic-DeepLab w/ SWideRNet [Cityscapes-fine] & 58,4 \\
	  Seamless Scene Segmentation & 57,7 \\
	  Axial-DeepLab-XL [Mapillary Vistas] & 57,2 \\
	  Unifying Training and Inference for Panoptic Segmentation [COCO] & 56,5 \\
	  kMaX-DeepLab [Cityscapes-fine] & 56 \\
	  Axial-DeepLab-L [Mapillary Vistas] & 55,9 \\
	  TASCNet-enhanced & 55,2 \\
	  \hline
	\end{tabular}
  \end{table}
  
  

